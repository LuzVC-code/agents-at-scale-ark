---
title: Langfuse Service - Sequence Diagram
description: Monitor and trace AI agent interactions with Langfuse integration sequence diagram
---

# Langfuse Service - Sequence Diagram

Code Version: v0.1.31  
Last updated on September 4, 2025

This document explains the sequence diagram for **Langfuse Service** integration in ARK, showing how telemetry data flows from ARK components to Langfuse for comprehensive observability and monitoring of AI agent interactions.

## System Overview

### Key Participants

1. **User** - Person or system submitting queries to ARK
2. **ARK Controller** - Core ARK controller managing query lifecycle
3. **OTEL Collector** - OpenTelemetry collector for telemetry data aggregation
4. **Langfuse Service** - Langfuse web service for observability and analytics
5. **Agent** - AI agent processing queries
6. **Model Client** - LLM model client (OpenAI, Azure, etc.)
7. **External LLM Provider** - AI model provider service
8. **Langfuse Database** - PostgreSQL database storing telemetry data

### Langfuse Integration Architecture

ARK's Langfuse integration provides:

#### Core Features
- **Automatic Telemetry Collection**: OpenTelemetry-based data collection from all ARK components
- **LLM Call Tracing**: Complete tracing of model interactions and API calls
- **Cost Monitoring**: Token usage and cost tracking across all models
- **Performance Analytics**: Response times, throughput, and latency metrics
- **Session Management**: Grouped interactions and conversation tracking
- **Error Tracking**: Comprehensive error monitoring and debugging

#### Telemetry Components
- **OTEL Headers**: Automatic configuration of OpenTelemetry headers
- **Trace Collection**: Distributed tracing across ARK services
- **Metric Aggregation**: Performance and usage metrics collection
- **Event Recording**: Structured event logging for observability

## Sequence Diagram

```mermaid
sequenceDiagram
    participant User as User
    participant Controller as ARK Controller
    participant OTEL as OTEL Collector
    participant Langfuse as Langfuse Service
    participant Agent as Agent
    participant ModelClient as Model Client
    participant LLM as External LLM Provider
    participant Database as Langfuse Database

    Note over User, Database: Langfuse Observability and Tracing Flow

    %% Phase 1: System Initialization and OTEL Configuration
    Controller->>Controller: Initialize ARK Controller with OTEL config
    Controller->>OTEL: Configure OTEL headers and endpoint
    OTEL->>Langfuse: Register telemetry endpoint: /api/public/otel
    Langfuse->>Database: Initialize session and project tracking
    
    Note over Controller, Database: Startup Trace: ARK Controller initialization

    %% Phase 2: Query Submission and Trace Initiation
    User->>Controller: Submit Query CRD
    Controller->>OTEL: StartTrace("QueryExecution", queryId, metadata)
    OTEL->>Langfuse: POST /api/public/otel (Trace start event)
    Langfuse->>Database: Store trace initiation with session info
    
    Controller->>Controller: Process Query CRD and extract requirements
    Controller->>OTEL: AddEvent("QueryProcessed", query details)
    OTEL->>Langfuse: POST /api/public/otel (Query processing event)
    
    %% Phase 3: Agent Resolution and Configuration
    Controller->>Controller: Resolve target agent configuration
    Controller->>OTEL: AddEvent("AgentResolved", agent metadata)
    Controller->>Agent: Initialize agent with model and prompt
    
    OTEL->>Langfuse: POST /api/public/otel (Agent resolution event)
    Langfuse->>Database: Store agent configuration and metadata
    
    %% Phase 4: Model Client Creation and LLM Call Preparation
    Agent->>ModelClient: Create model client with provider config
    Agent->>OTEL: StartSpan("LLMCall", model details, prompt)
    OTEL->>Langfuse: POST /api/public/otel (LLM call initiation)
    
    Agent->>Agent: Prepare prompt and messages for LLM
    Agent->>OTEL: AddEvent("PromptPrepared", prompt tokens, content)
    
    %% Phase 5: LLM Provider Interaction and Token Tracking
    ModelClient->>LLM: HTTP request to LLM provider (OpenAI, Azure, etc.)
    ModelClient->>OTEL: AddEvent("LLMRequestSent", request details)
    
    LLM-->>ModelClient: AI response with content and usage metadata
    ModelClient->>OTEL: AddEvent("LLMResponseReceived", tokens, cost)
    
    %% Phase 6: Response Processing and Metrics Collection
    ModelClient-->>Agent: LLM response with token usage
    Agent->>OTEL: EndSpan("LLMCall", response tokens, duration)
    Agent->>OTEL: AddMetrics("TokenUsage", prompt_tokens, completion_tokens)
    
    OTEL->>Langfuse: POST /api/public/otel (LLM completion with metrics)
    Langfuse->>Database: Store LLM interaction, tokens, and cost data
    
    %% Phase 7: Query Completion and Final Tracing
    Agent-->>Controller: Return processed response
    Controller->>OTEL: AddEvent("QueryCompleted", response details)
    Controller->>OTEL: EndTrace("QueryExecution", final_status)
    
    OTEL->>Langfuse: POST /api/public/otel (Query completion event)
    Langfuse->>Database: Finalize trace and session data
    
    Controller-->>User: Return query response
    
    %% Phase 8: Analytics and Dashboard Updates
    Langfuse->>Database: Aggregate metrics and performance data
    Langfuse->>Langfuse: Update dashboard views:
    Note over Langfuse: - Traces view with execution flow
    Note over Langfuse: - Sessions view with conversation history
    Note over Langfuse: - Models view with usage statistics
    Note over Langfuse: - Cost analysis and token breakdowns
    
    %% Phase 9: Error Handling and Monitoring (Alternative Flow)
    alt Error Scenarios
        alt LLM Provider Error
            ModelClient->>LLM: Request to LLM provider
            LLM-->>ModelClient: Error response (rate limit, API error, etc.)
            ModelClient->>OTEL: AddEvent("LLMError", error details)
            OTEL->>Langfuse: POST /api/public/otel (Error event)
            Langfuse->>Database: Store error information for debugging
            ModelClient-->>Agent: Error response
            Agent->>OTEL: EndSpan("LLMCall", error_status)
        else Controller Processing Error
            Controller->>Controller: Exception in query processing
            Controller->>OTEL: AddEvent("ControllerError", error details)
            OTEL->>Langfuse: POST /api/public/otel (Controller error)
            Langfuse->>Database: Store controller error for monitoring
            Controller->>OTEL: EndTrace("QueryExecution", error_status)
        else OTEL Collection Error
            OTEL->>Langfuse: POST /api/public/otel (telemetry data)
            Langfuse-->>OTEL: Error response (service unavailable)
            OTEL->>OTEL: Buffer telemetry data for retry
            Note over OTEL: Retry with exponential backoff
        end
    end

    %% Phase 10: Continuous Monitoring and Health Checks
    Note over Controller, Database: Continuous Monitoring and Health
    Controller->>OTEL: Periodic health check events
    OTEL->>Langfuse: POST /api/public/otel (Health status)
    Langfuse->>Database: Update service health metrics
    
    Langfuse->>Langfuse: Generate alerts for:
    Note over Langfuse: - High error rates
    Note over Langfuse: - Performance degradation
    Note over Langfuse: - Cost threshold breaches
    Note over Langfuse: - Token usage anomalies

    Note over User, Database: Complete Langfuse Observability Flow
```
## Langfuse Integration Flow Phases

### Phase 1: System Initialization and OTEL Configuration
- **OTEL Setup**: ARK Controller initializes with OpenTelemetry configuration
- **Endpoint Registration**: OTEL collector registers with Langfuse telemetry endpoint
- **Authentication**: Automatic configuration of API keys and authorization headers
- **Database Initialization**: Langfuse prepares session and project tracking
- **Startup Trace**: Initial trace recorded for ARK Controller startup

### Phase 2: Query Submission and Trace Initiation
- **Query Reception**: User submits Query CRD to ARK Controller
- **Trace Start**: New trace initiated with unique query ID and metadata
- **Event Recording**: Query processing events sent to Langfuse via OTEL
- **Session Management**: Query associated with user session for conversation tracking
- **Metadata Collection**: Query parameters and context stored for analysis

### Phase 3: Agent Resolution and Configuration
- **Agent Discovery**: ARK Controller resolves target agent configuration
- **Configuration Logging**: Agent metadata and configuration sent to Langfuse
- **Agent Initialization**: Agent initialized with model client and prompt
- **Telemetry Setup**: Agent configured for automatic telemetry collection
- **Context Preservation**: Agent context stored for trace continuity

### Phase 4: Model Client Creation and LLM Call Preparation
- **Client Creation**: Model client created with provider-specific configuration
- **Span Initiation**: New span started for LLM interaction tracking
- **Prompt Preparation**: User prompt and conversation history prepared
- **Token Estimation**: Initial token count estimation for cost tracking
- **Request Logging**: LLM request details logged to Langfuse

### Phase 5: LLM Provider Interaction and Token Tracking
- **API Request**: HTTP request sent to external LLM provider
- **Request Tracking**: Request details and timing recorded
- **Response Reception**: LLM response received with content and metadata
- **Token Calculation**: Actual token usage extracted from response
- **Cost Attribution**: Token costs calculated and attributed to query

### Phase 6: Response Processing and Metrics Collection
- **Response Processing**: LLM response processed and formatted
- **Span Completion**: LLM call span completed with final metrics
- **Usage Metrics**: Token usage, cost, and performance metrics recorded
- **Database Storage**: Complete LLM interaction stored in Langfuse database
- **Performance Tracking**: Response time and throughput metrics captured

### Phase 7: Query Completion and Final Tracing
- **Response Assembly**: Final response assembled and validated
- **Completion Events**: Query completion events recorded
- **Trace Finalization**: Complete trace finalized with final status
- **Session Update**: User session updated with query results
- **Response Delivery**: Final response delivered to user

### Phase 8: Analytics and Dashboard Updates
- **Metrics Aggregation**: Langfuse aggregates performance and usage metrics
- **Dashboard Updates**: Real-time updates to Langfuse dashboard views
- **Trace Visualization**: Complete execution trace available for analysis
- **Session Analytics**: Conversation-level analytics and insights
- **Cost Analysis**: Token usage and cost breakdowns by model and agent

### Phase 9: Error Handling and Monitoring
- **Error Detection**: Comprehensive error detection across all components
- **Error Logging**: Detailed error information sent to Langfuse
- **Debug Information**: Stack traces and context preserved for debugging
- **Alert Generation**: Automatic alerts for critical errors or thresholds
- **Recovery Tracking**: Error recovery attempts and outcomes logged

### Phase 10: Continuous Monitoring and Health Checks
- **Health Monitoring**: Continuous health checks across ARK services
- **Performance Tracking**: Ongoing performance and availability monitoring
- **Alert Management**: Automated alerting for anomalies and issues
- **Trend Analysis**: Long-term trends and patterns in usage and performance
- **Optimization Insights**: Data-driven insights for system optimization

## OTEL Configuration and Setup

### Automatic Configuration
```bash
# Environment variables automatically configured by make langfuse-install
OTEL_EXPORTER_OTLP_ENDPOINT=http://langfuse-web.telemetry.svc.cluster.local:3000/api/public/otel
OTEL_EXPORTER_OTLP_HEADERS=Authorization=Basic <base64-encoded-credentials>
```

### Kubernetes Deployment Configuration
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: otel-environment-variables
  namespace: ark-system
type: Opaque
stringData:
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://langfuse-web.telemetry.svc.cluster.local:3000/api/public/otel"
  OTEL_EXPORTER_OTLP_HEADERS: "Authorization=Basic bGZfcGtfMTIzNDU2Nzg5MDpsZl9za18xMjM0NTY3ODkw"
```

### Service Discovery
```yaml
apiVersion: v1
kind: Service
metadata:
  name: langfuse-web
  namespace: telemetry
spec:
  selector:
    app: langfuse
  ports:
    - name: http
      port: 3000
      targetPort: 3000
```

## Dashboard Views and Analytics

### Traces View
- **Complete Execution Traces**: End-to-end query execution visualization
- **Span Details**: Individual operation timing and metadata
- **Error Analysis**: Error rates, types, and debugging information
- **Performance Metrics**: Response times, latency, and throughput

### Sessions View
- **Conversation Tracking**: Grouped interactions and conversation history
- **User Journey**: Complete user interaction flows
- **Session Analytics**: Duration, query count, and engagement metrics
- **Context Preservation**: Conversation context and state tracking

### Models View
- **Usage Statistics**: Model usage patterns and frequency
- **Cost Breakdown**: Token costs by model, agent, and time period
- **Performance Comparison**: Model response times and quality metrics
- **Provider Analytics**: Performance across different LLM providers

### Monitoring and Alerts
- **Real-time Monitoring**: Live system health and performance dashboards
- **Threshold Alerts**: Automated alerts for performance and cost thresholds
- **Error Tracking**: Comprehensive error monitoring and debugging tools
- **Trend Analysis**: Historical trends and pattern recognition

## Performance and Cost Optimization

### Token Usage Optimization
- **Real-time Tracking**: Live token usage monitoring and cost calculation
- **Usage Patterns**: Analysis of token consumption patterns
- **Cost Attribution**: Token costs attributed to specific agents and queries
- **Optimization Insights**: Data-driven recommendations for cost reduction

### Performance Monitoring
- **Response Time Analysis**: Detailed response time breakdowns
- **Throughput Metrics**: Query processing throughput and capacity planning
- **Bottleneck Identification**: Performance bottleneck detection and analysis
- **Scalability Insights**: System scalability recommendations based on usage patterns

This comprehensive Langfuse integration provides ARK with world-class observability and monitoring capabilities while maintaining full traceability of AI agent interactions and costs.

Version: v0.1.31
### Local Development Access

The `make langfuse-dashboard` command automatically opens the dashboard and logs you in. Alternatively, you can access it manually:

```bash
# Option 1: minikube tunnel (exposes on standard ports)
minikube tunnel
# Access via: http://localhost/langfuse

# Option 2: port-forward nginx ingress (custom port)
kubectl port-forward service/nginx-ingress 8080:80 -n ark-system
# Access via: http://localhost:8080/langfuse

# Option 3: direct port-forward to langfuse service
kubectl port-forward service/langfuse-web 5264:3000 -n telemetry
# Access via: http://localhost:5264
```

### Gateway API Access

Langfuse is configured with Gateway API HTTPRoute for namespace-based access:

```bash
# Access via nip.io DNS pattern
open http://langfuse.telemetry.127.0.0.1.nip.io:8080
```

## Configuration

### OTEL Environment Variables

The installation automatically configures these OpenTelemetry environment variables:

```bash
# Automatically configured by make langfuse-install
OTEL_EXPORTER_OTLP_ENDPOINT=http://langfuse-web.telemetry.svc.cluster.local:3000/api/public/otel
OTEL_EXPORTER_OTLP_HEADERS=Authorization=Basic <base64-encoded-credentials>
```

### Service Configuration

Langfuse is deployed with these default settings:

- **Namespace**: `telemetry`
- **Service Name**: `langfuse-web`
- **Port**: `3000`
- **Public Key**: `lf_pk_1234567890`
- **Secret Key**: `lf_sk_1234567890`

### Automatic OTEL Header Deployment

The installation automatically deploys OTEL configuration to:
- `ark-system` namespace
- `default` namespace

This enables automatic telemetry collection from ARK services.

## Using Langfuse Dashboard

### Dashboard Overview

Once logged in, the Langfuse dashboard provides:

**Traces View**
- Complete execution traces for ARK controller operations
- Model calls and responses
- Timing and performance metrics

**Sessions View**
- Grouped interactions and conversations
- Session-level analytics

**Models View**
- Model usage statistics
- Cost breakdown by model
- Performance comparisons

### Viewing ARK Traces

After installation, you should immediately see a 'startup' trace from the ARK controller. This confirms that telemetry is working correctly.

Traces include:
- Controller startup events
- Query execution flows
- Model interactions
- Tool executions

## Troubleshooting

### Langfuse Service Issues

```bash
# Check Langfuse pod status
kubectl get pods -l app=langfuse -n telemetry

# View Langfuse logs
kubectl logs -l app=langfuse -n telemetry

# Check service connectivity
kubectl exec -it <ark-controller-pod> -- curl http://langfuse-web.telemetry.svc.cluster.local:3000/health
```

### OTEL Configuration Issues

```bash
# Check OTEL environment variables
kubectl get secret otel-environment-variables -n ark-system -o yaml

# Verify ARK controller has OTEL config
kubectl describe deployment ark-controller-manager -n ark-system | grep -A 10 envFrom

# Check telemetry initialization logs
kubectl logs deployment/ark-controller-manager -n ark-system | grep telemetry
```

### Dashboard Access Issues

```bash
# Check if Langfuse is ready
kubectl get deployment langfuse-web -n telemetry

# Test direct port-forward access
kubectl port-forward service/langfuse-web 5264:3000 -n telemetry

# Check Gateway API configuration
kubectl get httproute langfuse-telemetry -n telemetry -o yaml
```

## Uninstalling

To remove Langfuse and clean up OTEL configuration:

```bash
# Uninstall Langfuse and clean up OTEL secrets
make langfuse-uninstall
```

This will:
1. Remove Langfuse from the `telemetry` namespace
2. Delete OTEL environment variable secrets from `ark-system` namespace
3. Clean up installation stamps
